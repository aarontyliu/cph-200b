{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import lifelines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"~/Data/CPH200B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_1_data_dir = DATA_DIR / \"Project 1\"\n",
    "project_2_data_dir = DATA_DIR / \"Project 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Warm-up Exercise: Hypothesis Testing & Confounding [6 pts]\n",
    "\n",
    "The most basic form of causal inference involves comparing survival curves in different groups stratified by an\n",
    "intervention of interest. In this task, we will implement hypothesis testing methods to examine whether differences between the outcomes of treated and untreated patients are statistically significant, and whether these\n",
    "difference reflect the causal effect of the intervention.\n",
    "\n",
    "For all the tasks below, we will use the UNOS heart transplant [1] dataset from Project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos_data_filepath = project_1_data_dir / \"UNOS_train.csv\"\n",
    "\n",
    "unos_data = pd.read_csv(unos_data_filepath)\n",
    "unos_data[\"event\"] = unos_data['Censor (Censor = 1)'] == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.1 [1 pts]. \n",
    "Implement the Log-Rank test from scratch in Python. Using the UNOS dataset, apply\n",
    "your implemented test to check whether the survival outcomes of patients on ventricular assist device (VAD)\n",
    "support differ from those of patients without VAD support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rank_test(time_A, time_B, event_A, event_B):\n",
    "    # Combine data into a single DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": np.concatenate((time_A, time_B)),\n",
    "            \"event\": np.concatenate((event_A, event_B)),\n",
    "            \"group\": np.concatenate(\n",
    "                (np.zeros_like(time_A), np.ones_like(time_B))\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Sort by time\n",
    "    df = df.sort_values(by=\"time\")\n",
    "\n",
    "    # Compute risk set counts efficiently using cumulative sums\n",
    "    df[\"R_A\"] = (df[\"group\"] == 0)[::-1].cumsum()[::-1]  # Risk set for group A\n",
    "    df[\"R_B\"] = (df[\"group\"] == 1)[::-1].cumsum()[::-1]  # Risk set for group B\n",
    "    df[\"R\"] = df[\"R_A\"] + df[\"R_B\"]  # Total risk set\n",
    "\n",
    "    # Events per time point\n",
    "    df[\"d_A\"] = ((df[\"group\"] == 0) & (df[\"event\"] == 1)).astype(int)\n",
    "    df[\"d_B\"] = ((df[\"group\"] == 1) & (df[\"event\"] == 1)).astype(int)\n",
    "    df[\"d\"] = df[\"d_A\"] + df[\"d_B\"]\n",
    "\n",
    "    # Compute expected events\n",
    "    df[\"E_A\"] = df[\"d\"] * (df[\"R_A\"] / df[\"R\"])\n",
    "    df[\"E_B\"] = df[\"d\"] * (df[\"R_B\"] / df[\"R\"])\n",
    "\n",
    "    # Compute variance contributions\n",
    "    df[\"V\"] = (df[\"R_A\"] * df[\"R_B\"] * df[\"d\"] * (df[\"R\"] - df[\"d\"])) / (\n",
    "        df[\"R\"] ** 2 * (df[\"R\"] - 1)\n",
    "    )\n",
    "    df[\"V\"] = df[\"V\"].fillna(0)  # Handle division by zero cases\n",
    "\n",
    "    # Compute log-rank test statistic\n",
    "    O_A, E_A, V = df[\"d_A\"].sum(), df[\"E_A\"].sum(), df[\"V\"].sum()\n",
    "    Z = (O_A - E_A) ** 2 / V if V > 0 else 0\n",
    "\n",
    "    # Compute p-value from chi-square distribution (df=1)\n",
    "    p_value = 1 - chi2.cdf(Z, df=1)\n",
    "\n",
    "    return Z, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-statistic: 57.830055054806515\n",
      "P-value: 2.853273173286652e-14\n",
      "P-values are close.\n"
     ]
    }
   ],
   "source": [
    "event_col = \"event\"\n",
    "time_col = \"Survival Time\"\n",
    "\n",
    "groups = unos_data.groupby(\"vad_while_listed\")\n",
    "\n",
    "duration_a = groups.get_group(0)[time_col].values\n",
    "duration_b = groups.get_group(1)[time_col].values\n",
    "\n",
    "event_a = groups.get_group(0)[event_col].values\n",
    "event_b = groups.get_group(1)[event_col].values\n",
    "\n",
    "z, pval = log_rank_test(duration_a, duration_b, event_a, event_b)\n",
    "print(f\"Z-statistic: {z}\")\n",
    "print(f\"P-value: {pval}\")\n",
    "\n",
    "#\n",
    "pval_from_lifelines = lifelines.statistics.logrank_test(duration_a, duration_b, event_a, event_b).p_value\n",
    "\n",
    "if np.isclose(pval, pval_from_lifelines):\n",
    "    print(\"P-values are close.\")\n",
    "else:\n",
    "    print(\"P-values are not close.\")\n",
    "    print(f\"P-value from lifelines: {pval_from_lifelines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.2 [1 pts]. \n",
    "Propose a method to determine if there are confounders in the UNOS dataset for the effect of VAD support on survival outcomes. List all detected confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find confounders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.3 [2 pts]. \n",
    "For the comparison of survival curves to have a causal interpretation, we need to adjust for confounding variables that may cause the patient groups being compared to have different clinical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.4 [2 pts]. \n",
    "Propose a propensity-weighted version of the Kaplan-Meier estimator you implemented in Project 1 that adjusts for confounding. Plot the propensity-weighted Kaplan-Meier curves in patients with and without VAD.\n",
    "Compare this plot with the survival curves of both groups using the standard Kaplan-Meier estimators.\n",
    "\n",
    "Propose a propensity-weighted version of the Long-Rank test. Apply this test to check whether the survival outcomes of patients on VAD support differ from those of patients without VAD. Com-\n",
    "pare the result of this test with the unadjusted test you implemented in Task 2.1.1. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ML-based Estimation of Average Treatment Effects [6 pts]\n",
    "\n",
    "### 2.2.1 Clinical Background, Dataset, and Setup\n",
    "\n",
    "In this task, we will use individual patient data from the International Stroke Trial (IST), one of the largest\n",
    "randomized trials ever conducted in acute stroke [2]. The trial investigated the impact of aspirin and subcuta-\n",
    "neous heparin on patients with acute ischemic stroke, with treatment randomization within 48 hours of symp-\n",
    "tom onset. The trial findings indicated no effect of both aspirin and heparin on 14-day and 6-month mortality.\n",
    "The trial protocol and data dictionary have been provided to you.\n",
    "The original IST data lacks confounding as it was generated through a randomized trial. The instructor intro-\n",
    "duced confounding artificially by filtering patients out of the trial using a random function that depends on the\n",
    "patient features. The resulting dataset mimics an observational dataset where treatment is assigned through\n",
    "a mechanism that depends on patient features. You will conduct the following tasks using the artificially\n",
    "confounded dataset with the goal of recovering the same treatment effects estimated in the randomized trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the average effect of aspirin and heparin on 14-day mortality using the following estimators. Compare your estimates with those of the original trial and provide commentary on the results.\n",
    "\n",
    "### Task 2.2.1 [1 pts]. \n",
    "A standard difference-in-means estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.2 [1 pts]. \n",
    "An inverse propensity weighting (IPW) estimator using a Gradient Boosting model for the propensity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.3 [2 pts]. \n",
    "A covariate adjustment estimator using a Gradient Boosting model with T-learner, S-learner, and X-learner architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.4 [2 pts]. \n",
    "An augmented IPW (doubly-robust) estimator that combines the propensity model from\n",
    "Task 2.2.2 and an outcomes model based on the S-learner in Task 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Counterfactual Inference and Domain Adaptation [8 pts]\n",
    "\n",
    "In this task, we will explore the application of concepts from the machine learning literature to estimate het-\n",
    "erogeneous treatment effects. The seminal work in [3] establishes a link between estimating treatment effects\n",
    "and the domain adaptation problem in machine learning. Using this insight, the authors repurpose ideas from\n",
    "domain adaptation literature to create a new deep learning model for estimating the conditional average treat-\n",
    "ment effects (CATE) function. The core idea of their algorithm is to eliminate confounding bias by learning a\n",
    "representation Φ of the features X that aligns the distribution of treated and control populations, Φ(X|T = 1)\n",
    "and Φ(X|T = 0), in the representation space, referred to by the authors as a “balancing” representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the paper carefully and complete the following tasks.\n",
    "\n",
    "### Task 2.3.1 [3 pts]. \n",
    "\n",
    "Implement the T ARN et and CF RM M D models proposed in [3] in PyTorch. Evaluate\n",
    "the performance of all models using the semi-synthetic benchmark dataset included in the Project 2 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3.2 [1 pts]. \n",
    "\n",
    "Visualize the treated and control features before and after applying the balancing representation Φ(.) using t-SNE. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3.3 [1 pts]. \n",
    "\n",
    "Show the impact of the scaling parameter α (Eq. (3) in [3]) on the loss function on the\n",
    "test set for the Maximum Mean Discrepancy (MMD) regularizer.\n",
    "\n",
    "### Task 2.3.4 [3 pts].\n",
    "\n",
    "Use the TARNet and CFR<sub>MMD</sub> models to estimate average treatment effects using\n",
    "the IST data in Task 2.2. Assess the alignment of your estimates with the trial results and compare them\n",
    "to the estimators in Tasks 2.2.3 and 2.2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 NeurIPS Reviewer for a Day: Reviewing & Reproducing Recent Research on ML-Based Causal Inference [10 pts]\n",
    "In this task, we will focus on one paper that proposes new methods for estimating CATE inspired by ideas we studied in Lectures 7, 8 and 9. The paper is “Adapting Neural Networks for the Estimation of Treatment\n",
    "Effects” by Claudia Shi, David Blei and Victor Veitch, which was published in NeurIPS 2019. The objective of this task is to develop critical paper review skills and practice reproducing research results. Please read\n",
    "the paper carefully and complete the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4.1 [5 pts]. \n",
    "\n",
    "Please review the NeurIPS 2024 reviewing guidelines and write a comprehensive review of this paper in accordance with those guidelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2.4.2 [5 pts]. \n",
    "\n",
    "Implement the DragonNet and Targeted regularization methods proposed in this paper in PyTorch and reproduce their performance results on the IHDP dataset (Table 1 in the paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cph200b-project-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
